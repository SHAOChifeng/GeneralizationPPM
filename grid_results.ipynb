{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate fitness, precision, generalization and average for the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "emb_size = math.ceil(20**0.25)\n",
    "\n",
    "print(emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappingfilename = 'Parallel/mapping.txt' \n",
    "with open(mappingfilename) as f:\n",
    "    mapping = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E2': 1, 'E1': 2, 'E3': 3, 'E4': 4, 'G': 5, 'A': 6, 'B': 7, 'C': 8, 'I': 9, 'F': 10, 'E5': 11, 'H': 12, 'D': 13}\n"
     ]
    }
   ],
   "source": [
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_log(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    return(df.values.tolist())\n",
    "\n",
    "def remove_nan(lists):\n",
    "    newlists = []\n",
    "    for tr in lists:\n",
    "        newlists.append([int(x) for x in tr if str(x) != 'nan'])\n",
    "    return(newlists)\n",
    "\n",
    "\n",
    "\n",
    "def delete_variant(log, variant):\n",
    "    return([trace for trace in log if trace != variant])\n",
    "\n",
    "def get_variants_list(lst): #get all of the variants in a list, return as list\n",
    "    st = set(tuple(i) for i in lst) #convert list into set of tuples\n",
    "    lst2 = list(st) #convert set of tuples into lsit of tuples\n",
    "    return [list(e) for e in lst2] \n",
    "\n",
    "def count_variant(log, variant): #count how many times a variant comes up in list\n",
    "    c = 0\n",
    "    for trace in log:\n",
    "        if trace == variant:\n",
    "            c += 1\n",
    "    return(c)\n",
    "\n",
    "def compare_variants(var1, var2): #compare two logs, what comes up in the other \n",
    "    s1 = set(tuple(i) for i in var1)\n",
    "    s2 = set(tuple(i) for i in var2)\n",
    "    \n",
    "   # print(\"Missing values in second list:\", (s1.difference(s2))) \n",
    "   # print(\"Additional values in second list:\", (s2.difference(s1))) \n",
    "    \n",
    "    return([list(e) for e in list(s1.difference(s2))],[list(e) for e in list(s2.difference(s1))])\n",
    "\n",
    "def demap_trace(t, mapping): #unmap trace, from number encoding to activity label\n",
    "    map = {v: k for k, v in mapping.items()}\n",
    "    return [map[a] for a in t]\n",
    "\n",
    "\n",
    "\n",
    "def apply_integer_map(log, map):\n",
    "    return [[map[a['concept:name']] for a in t] for t in log]\n",
    "\n",
    "#fucntion gets the counts of all of the variants \n",
    "\n",
    "def get_counts(log, variants):\n",
    "    counts = []\n",
    "    for var in variants:\n",
    "        counts.append(count_variant(log, var))\n",
    "    return counts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitness(occ_each_trvar_sim, occ_each_trvar_tr):\n",
    "    arr = [min(occ_each_trvar_sim[i], occ_each_trvar_tr[i])/sum(occ_each_trvar_tr) for i in range(0, len(occ_each_trvar_sim))]\n",
    "    return sum(arr)\n",
    "\n",
    "def get_precision(occ_each_simvar_sim, occ_each_simvar_trte):\n",
    "    arr = [min(occ_each_simvar_sim[i], occ_each_simvar_trte[i])/sum(occ_each_simvar_sim) for i in range(0, len(occ_each_simvar_sim))]\n",
    "    return sum(arr)\n",
    "\n",
    "def get_generalization(occ_each_tevar_sim, occ_each_tevar_te):\n",
    "    arr = [min(occ_each_tevar_sim[i], occ_each_tevar_te[i])/sum(occ_each_tevar_te) for i in range(0, len(occ_each_tevar_sim))]\n",
    "    return sum(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = remove_nan(import_log('Parallel/variants.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Embedding','Num_layers', 'Size', 'Dropout', 'Reg', 'Gen.', 'Prec.', 'Fit.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 16 0.0 0.0\n",
      "1 16 0.0 1e-05\n",
      "1 16 0.0 0.0001\n",
      "1 16 0.0 0.001\n",
      "1 16 0.0 0.01\n",
      "1 16 0.2 0.0\n",
      "1 16 0.2 1e-05\n",
      "1 16 0.2 0.0001\n",
      "1 16 0.2 0.001\n",
      "1 16 0.2 0.01\n",
      "1 16 0.4 0.0\n",
      "1 16 0.4 1e-05\n",
      "1 16 0.4 0.0001\n",
      "1 16 0.4 0.001\n",
      "1 16 0.4 0.01\n",
      "1 32 0.0 0.0\n",
      "1 32 0.0 1e-05\n",
      "1 32 0.0 0.0001\n",
      "1 32 0.0 0.001\n",
      "1 32 0.0 0.01\n",
      "1 32 0.2 0.0\n",
      "1 32 0.2 1e-05\n",
      "1 32 0.2 0.0001\n",
      "1 32 0.2 0.001\n",
      "1 32 0.2 0.01\n",
      "1 32 0.4 0.0\n",
      "1 32 0.4 1e-05\n",
      "1 32 0.4 0.0001\n",
      "1 32 0.4 0.001\n",
      "1 32 0.4 0.01\n",
      "1 64 0.0 0.0\n",
      "1 64 0.0 1e-05\n",
      "1 64 0.0 0.0001\n",
      "1 64 0.0 0.001\n",
      "1 64 0.0 0.01\n",
      "1 64 0.2 0.0\n",
      "1 64 0.2 1e-05\n",
      "1 64 0.2 0.0001\n",
      "1 64 0.2 0.001\n",
      "1 64 0.2 0.01\n",
      "1 64 0.4 0.0\n",
      "1 64 0.4 1e-05\n",
      "1 64 0.4 0.0001\n",
      "1 64 0.4 0.001\n",
      "1 64 0.4 0.01\n",
      "2 16 0.0 0.0\n",
      "2 16 0.0 1e-05\n",
      "2 16 0.0 0.0001\n",
      "2 16 0.0 0.001\n",
      "2 16 0.0 0.01\n",
      "2 16 0.2 0.0\n",
      "2 16 0.2 1e-05\n",
      "2 16 0.2 0.0001\n",
      "2 16 0.2 0.001\n",
      "2 16 0.2 0.01\n",
      "2 16 0.4 0.0\n",
      "2 16 0.4 1e-05\n",
      "2 16 0.4 0.0001\n",
      "2 16 0.4 0.001\n",
      "2 16 0.4 0.01\n",
      "2 32 0.0 0.0\n",
      "2 32 0.0 1e-05\n",
      "2 32 0.0 0.0001\n",
      "2 32 0.0 0.001\n",
      "2 32 0.0 0.01\n",
      "2 32 0.2 0.0\n",
      "2 32 0.2 1e-05\n",
      "2 32 0.2 0.0001\n",
      "2 32 0.2 0.001\n",
      "2 32 0.2 0.01\n",
      "2 32 0.4 0.0\n",
      "2 32 0.4 1e-05\n",
      "2 32 0.4 0.0001\n",
      "2 32 0.4 0.001\n",
      "2 32 0.4 0.01\n",
      "2 64 0.0 0.0\n",
      "2 64 0.0 1e-05\n",
      "2 64 0.0 0.0001\n",
      "2 64 0.0 0.001\n",
      "2 64 0.0 0.01\n",
      "2 64 0.2 0.0\n",
      "2 64 0.2 1e-05\n",
      "2 64 0.2 0.0001\n",
      "2 64 0.2 0.001\n",
      "2 64 0.2 0.01\n",
      "2 64 0.4 0.0\n",
      "2 64 0.4 1e-05\n",
      "2 64 0.4 0.0001\n",
      "2 64 0.4 0.001\n",
      "2 64 0.4 0.01\n"
     ]
    }
   ],
   "source": [
    "grid_nr_layers =  [1, 2]\n",
    "grid_layersize = [16, 32, 64]\n",
    "grid_dropout = [0.0, 0.2, 0.4]\n",
    "grid_reg = [0.0, 0.00001, 0.0001, 0.001, 0.01]\n",
    "\n",
    "for num_layers in grid_nr_layers:\n",
    "    for layersize in grid_layersize:\n",
    "        for dropout in grid_dropout:\n",
    "            for reg in grid_reg:\n",
    "                \n",
    "                fitness_arr = []\n",
    "                precision_arr = []\n",
    "                generalization_arr = []\n",
    "                \n",
    "                \n",
    "                print(num_layers, layersize, dropout, reg)\n",
    "                \n",
    "                for variant in range(1,9):\n",
    "                    trainname = 'Parallel/Training_logs/log_'+str(variant)+'.csv'\n",
    "                    trainlog = remove_nan(import_log(trainname))\n",
    "                    \n",
    "                    SimLogName = 'Parallel/Simulated_logs/Var'+str(variant)+'/SIMLOG_NL'+str(num_layers)+'emb'+'Y'+'LS'+str(layersize)+'D'+str(dropout).replace('0.', '')+'R'+str(reg).replace('.', '')+'.csv'\n",
    "                    simlog = remove_nan(import_log(SimLogName))\n",
    "                    \n",
    "                    #need to do lines below because didn't save testlog explicilty in setting up leave-one-out\n",
    "                    traintestlog = copy.deepcopy(trainlog)\n",
    "                    testlog = []\n",
    "                    for i in range(0, 12000-len(trainlog)):\n",
    "                        testlog.append(variants[variant])\n",
    "                        traintestlog.append(variants[variant])\n",
    "                    \n",
    "                    trvar = get_variants_list(trainlog)\n",
    "                    simvar = get_variants_list(simlog)\n",
    "                    tevar = [variants[variant]] #this is unique tot eh one-hot seting, needs to be altered when using bigger test set\n",
    "                    \n",
    "                    #get counts for the simulated log\n",
    "                    occ_each_trvar_sim = get_counts(simlog, trvar)\n",
    "                    occ_each_tevar_sim = get_counts(simlog, tevar)\n",
    "                    occ_each_simvar_sim = get_counts(simlog, simvar)\n",
    "                    \n",
    "                    #get counts for the train log\n",
    "                    occ_each_trvar_tr = get_counts(trainlog, trvar)\n",
    "\n",
    "                    #get counts for the test log\n",
    "                    occ_each_tevar_te = get_counts(testlog, tevar)\n",
    "                    \n",
    "                    #get counts for the train test log\n",
    "                    occ_each_simvar_trte = get_counts(traintestlog, simvar)\n",
    "\n",
    "                    fitness_arr.append(get_fitness(occ_each_trvar_sim, occ_each_trvar_tr))\n",
    "                    precision_arr.append(get_precision(occ_each_simvar_sim, occ_each_simvar_trte))\n",
    "                    generalization_arr.append(get_generalization(occ_each_tevar_sim, occ_each_tevar_te))\n",
    "                \n",
    "                av_gen = sum(generalization_arr)/8.0\n",
    "                av_prec = sum(precision_arr)/8.0\n",
    "                av_fit = sum(fitness_arr)/8.0\n",
    "                \n",
    "                new_row = {'Embedding':'Yes','Num_layers':num_layers, 'Size':layersize, 'Dropout':dropout, 'Reg':reg, 'Gen.':av_gen, 'Prec.':av_prec, 'Fit.':av_fit}\n",
    "                \n",
    "                df = df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 16 0.0 0.0\n",
      "1 16 0.0 1e-05\n",
      "1 16 0.0 0.0001\n",
      "1 16 0.0 0.001\n",
      "1 16 0.0 0.01\n",
      "1 16 0.2 0.0\n",
      "1 16 0.2 1e-05\n",
      "1 16 0.2 0.0001\n",
      "1 16 0.2 0.001\n",
      "1 16 0.2 0.01\n",
      "1 16 0.4 0.0\n",
      "1 16 0.4 1e-05\n",
      "1 16 0.4 0.0001\n",
      "1 16 0.4 0.001\n",
      "1 16 0.4 0.01\n",
      "1 32 0.0 0.0\n",
      "1 32 0.0 1e-05\n",
      "1 32 0.0 0.0001\n",
      "1 32 0.0 0.001\n",
      "1 32 0.0 0.01\n",
      "1 32 0.2 0.0\n",
      "1 32 0.2 1e-05\n",
      "1 32 0.2 0.0001\n",
      "1 32 0.2 0.001\n",
      "1 32 0.2 0.01\n",
      "1 32 0.4 0.0\n",
      "1 32 0.4 1e-05\n",
      "1 32 0.4 0.0001\n",
      "1 32 0.4 0.001\n",
      "1 32 0.4 0.01\n",
      "1 64 0.0 0.0\n",
      "1 64 0.0 1e-05\n",
      "1 64 0.0 0.0001\n",
      "1 64 0.0 0.001\n",
      "1 64 0.0 0.01\n",
      "1 64 0.2 0.0\n",
      "1 64 0.2 1e-05\n",
      "1 64 0.2 0.0001\n",
      "1 64 0.2 0.001\n",
      "1 64 0.2 0.01\n",
      "1 64 0.4 0.0\n",
      "1 64 0.4 1e-05\n",
      "1 64 0.4 0.0001\n",
      "1 64 0.4 0.001\n",
      "1 64 0.4 0.01\n",
      "2 16 0.0 0.0\n",
      "2 16 0.0 1e-05\n",
      "2 16 0.0 0.0001\n",
      "2 16 0.0 0.001\n",
      "2 16 0.0 0.01\n",
      "2 16 0.2 0.0\n",
      "2 16 0.2 1e-05\n",
      "2 16 0.2 0.0001\n",
      "2 16 0.2 0.001\n",
      "2 16 0.2 0.01\n",
      "2 16 0.4 0.0\n",
      "2 16 0.4 1e-05\n",
      "2 16 0.4 0.0001\n",
      "2 16 0.4 0.001\n",
      "2 16 0.4 0.01\n",
      "2 32 0.0 0.0\n",
      "2 32 0.0 1e-05\n",
      "2 32 0.0 0.0001\n",
      "2 32 0.0 0.001\n",
      "2 32 0.0 0.01\n",
      "2 32 0.2 0.0\n",
      "2 32 0.2 1e-05\n",
      "2 32 0.2 0.0001\n",
      "2 32 0.2 0.001\n",
      "2 32 0.2 0.01\n",
      "2 32 0.4 0.0\n",
      "2 32 0.4 1e-05\n",
      "2 32 0.4 0.0001\n",
      "2 32 0.4 0.001\n",
      "2 32 0.4 0.01\n",
      "2 64 0.0 0.0\n",
      "2 64 0.0 1e-05\n",
      "2 64 0.0 0.0001\n",
      "2 64 0.0 0.001\n",
      "2 64 0.0 0.01\n",
      "2 64 0.2 0.0\n",
      "2 64 0.2 1e-05\n",
      "2 64 0.2 0.0001\n",
      "2 64 0.2 0.001\n",
      "2 64 0.2 0.01\n",
      "2 64 0.4 0.0\n",
      "2 64 0.4 1e-05\n",
      "2 64 0.4 0.0001\n",
      "2 64 0.4 0.001\n",
      "2 64 0.4 0.01\n"
     ]
    }
   ],
   "source": [
    "grid_nr_layers =  [1, 2]\n",
    "grid_layersize = [16, 32, 64]\n",
    "grid_dropout = [0.0, 0.2, 0.4]\n",
    "grid_reg = [0.0, 0.00001, 0.0001, 0.001, 0.01]\n",
    "\n",
    "for num_layers in grid_nr_layers:\n",
    "    for layersize in grid_layersize:\n",
    "        for dropout in grid_dropout:\n",
    "            for reg in grid_reg:\n",
    "                \n",
    "                fitness_arr = []\n",
    "                precision_arr = []\n",
    "                generalization_arr = []\n",
    "                \n",
    "                \n",
    "                print(num_layers, layersize, dropout, reg)\n",
    "                \n",
    "                for variant in range(1,9):\n",
    "                    trainname = 'Parallel/Training_logs/log_'+str(variant)+'.csv'\n",
    "                    trainlog = remove_nan(import_log(trainname))\n",
    "                    \n",
    "                    SimLogName = 'Parallel/Simulated_logs/Var'+str(variant)+'/SIMLOG_NL'+str(num_layers)+'emb'+'N'+'LS'+str(layersize)+'D'+str(dropout).replace('0.', '')+'R'+str(reg).replace('.', '')+'.csv'\n",
    "                    simlog = remove_nan(import_log(SimLogName))\n",
    "                    \n",
    "                    #need to do lines below because didn't save testlog explicilty in setting up leave-one-out\n",
    "                    traintestlog = copy.deepcopy(trainlog)\n",
    "                    testlog = []\n",
    "                    for i in range(0, 12000-len(trainlog)):\n",
    "                        testlog.append(variants[variant])\n",
    "                        traintestlog.append(variants[variant])\n",
    "                    \n",
    "                    trvar = get_variants_list(trainlog)\n",
    "                    simvar = get_variants_list(simlog)\n",
    "                    tevar = [variants[variant]] #this is unique tot eh one-hot seting, needs to be altered when using bigger test set\n",
    "                    \n",
    "                    #get counts for the simulated log\n",
    "                    occ_each_trvar_sim = get_counts(simlog, trvar)\n",
    "                    occ_each_tevar_sim = get_counts(simlog, tevar)\n",
    "                    occ_each_simvar_sim = get_counts(simlog, simvar)\n",
    "                    \n",
    "                    #get counts for the train log\n",
    "                    occ_each_trvar_tr = get_counts(trainlog, trvar)\n",
    "\n",
    "                    #get counts for the test log\n",
    "                    occ_each_tevar_te = get_counts(testlog, tevar)\n",
    "                    \n",
    "                    #get counts for the train test log\n",
    "                    occ_each_simvar_trte = get_counts(traintestlog, simvar)\n",
    "\n",
    "                    fitness_arr.append(get_fitness(occ_each_trvar_sim, occ_each_trvar_tr))\n",
    "                    precision_arr.append(get_precision(occ_each_simvar_sim, occ_each_simvar_trte))\n",
    "                    generalization_arr.append(get_generalization(occ_each_tevar_sim, occ_each_tevar_te))\n",
    "                \n",
    "                av_gen = sum(generalization_arr)/8.0\n",
    "                av_prec = sum(precision_arr)/8.0\n",
    "                av_fit = sum(fitness_arr)/8.0\n",
    "                \n",
    "                new_row = {'Embedding':'No','Num_layers':num_layers, 'Size':layersize, 'Dropout':dropout, 'Reg':reg, 'Gen.':av_gen, 'Prec.':av_prec, 'Fit.':av_fit}\n",
    "                \n",
    "                df = df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average'] = df[['Gen.', 'Prec.', 'Fit.']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prec = df.sort_values('Prec.', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen = df.sort_values('Gen.', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fit = df.sort_values('Fit.', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_average = df.sort_values('average', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Embedding Num_layers Size  Dropout      Reg      Gen.     Prec.      Fit.  \\\n",
      "105        No          1   32      0.0  0.00000  0.008831  0.950646  0.958492   \n",
      "30        Yes          1   64      0.0  0.00000  0.005435  0.949031  0.956883   \n",
      "80        Yes          2   64      0.2  0.00000  0.000000  0.948833  0.956738   \n",
      "45        Yes          2   16      0.0  0.00000  0.001238  0.948781  0.956675   \n",
      "60        Yes          2   32      0.0  0.00000  0.002567  0.948313  0.956190   \n",
      "15        Yes          1   32      0.0  0.00000  0.001190  0.948052  0.955940   \n",
      "135        No          2   16      0.0  0.00000  0.001087  0.948000  0.955887   \n",
      "21        Yes          1   32      0.2  0.00001  0.050373  0.948354  0.955832   \n",
      "166        No          2   64      0.0  0.00001  0.031826  0.947719  0.955350   \n",
      "40        Yes          1   64      0.4  0.00000  0.000000  0.947406  0.955299   \n",
      "\n",
      "      average  \n",
      "105  0.639323  \n",
      "30   0.637116  \n",
      "80   0.635191  \n",
      "45   0.635565  \n",
      "60   0.635690  \n",
      "15   0.635061  \n",
      "135  0.634991  \n",
      "21   0.651520  \n",
      "166  0.644965  \n",
      "40   0.634235  \n"
     ]
    }
   ],
   "source": [
    "print(df_fit[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Embedding Num_layers Size  Dropout     Reg      Gen.     Prec.      Fit.  \\\n",
      "55        Yes          2   16      0.4  0.0000  0.504598  0.918760  0.922243   \n",
      "147        No          2   16      0.4  0.0001  0.719339  0.915729  0.917367   \n",
      "109        No          1   32      0.0  0.0100  0.742637  0.913760  0.915238   \n",
      "94         No          1   16      0.0  0.0100  0.754468  0.913094  0.914504   \n",
      "114        No          1   32      0.2  0.0100  0.780081  0.913000  0.914145   \n",
      "19        Yes          1   32      0.0  0.0100  0.623784  0.911073  0.913572   \n",
      "124        No          1   64      0.0  0.0100  0.772850  0.908427  0.909606   \n",
      "4         Yes          1   16      0.0  0.0100  0.713283  0.907292  0.908992   \n",
      "12        Yes          1   16      0.4  0.0001  0.556550  0.905354  0.908245   \n",
      "34        Yes          1   64      0.0  0.0100  0.650104  0.897927  0.900037   \n",
      "57        Yes          2   16      0.4  0.0001  0.697872  0.889823  0.891502   \n",
      "168        No          2   64      0.0  0.0010  0.597618  0.867198  0.869433   \n",
      "23        Yes          1   32      0.2  0.0010  0.704382  0.855458  0.856768   \n",
      "129        No          1   64      0.2  0.0100  0.745886  0.810229  0.810819   \n",
      "153        No          2   32      0.0  0.0010  0.536531  0.805760  0.808019   \n",
      "99         No          1   16      0.2  0.0100  0.733395  0.775406  0.775775   \n",
      "138        No          2   16      0.0  0.0010  0.531767  0.770010  0.771998   \n",
      "134        No          1   64      0.4  0.0100  0.717175  0.752125  0.752445   \n",
      "8         Yes          1   16      0.2  0.0010  0.564665  0.704208  0.705441   \n",
      "158        No          2   32      0.2  0.0010  0.522315  0.686438  0.687820   \n",
      "143        No          2   16      0.2  0.0010  0.510343  0.623594  0.624593   \n",
      "119        No          1   32      0.4  0.0100  0.467280  0.537813  0.538425   \n",
      "28        Yes          1   32      0.4  0.0010  0.310005  0.485510  0.486961   \n",
      "178        No          2   64      0.4  0.0010  0.396941  0.476896  0.477551   \n",
      "43        Yes          1   64      0.4  0.0010  0.350452  0.464271  0.465304   \n",
      "13        Yes          1   16      0.4  0.0010  0.260643  0.376625  0.377638   \n",
      "68        Yes          2   32      0.2  0.0010  0.234686  0.311146  0.311847   \n",
      "39        Yes          1   64      0.2  0.0100  0.234781  0.300656  0.301207   \n",
      "24        Yes          1   32      0.2  0.0100  0.242744  0.293198  0.293681   \n",
      "53        Yes          2   16      0.2  0.0010  0.268586  0.283646  0.283776   \n",
      "9         Yes          1   16      0.2  0.0100  0.202552  0.273292  0.273886   \n",
      "163        No          2   32      0.4  0.0010  0.239473  0.257333  0.257487   \n",
      "44        Yes          1   64      0.4  0.0100  0.152760  0.239594  0.240334   \n",
      "78        Yes          2   64      0.0  0.0010  0.160700  0.231094  0.231640   \n",
      "29        Yes          1   32      0.4  0.0100  0.202662  0.230813  0.231069   \n",
      "83        Yes          2   64      0.2  0.0010  0.162763  0.228979  0.229612   \n",
      "14        Yes          1   16      0.4  0.0100  0.171373  0.205552  0.205844   \n",
      "104        No          1   16      0.4  0.0100  0.142705  0.158083  0.158219   \n",
      "58        Yes          2   16      0.4  0.0010  0.105438  0.136010  0.136266   \n",
      "73        Yes          2   32      0.4  0.0010  0.114501  0.127250  0.127363   \n",
      "148        No          2   16      0.4  0.0010  0.096001  0.117146  0.117334   \n",
      "88        Yes          2   64      0.4  0.0010  0.034371  0.039188  0.039230   \n",
      "154        No          2   32      0.0  0.0100  0.000000  0.000000  0.000000   \n",
      "74        Yes          2   32      0.4  0.0100  0.000000  0.000000  0.000000   \n",
      "49        Yes          2   16      0.0  0.0100  0.000000  0.000000  0.000000   \n",
      "54        Yes          2   16      0.2  0.0100  0.000000  0.000000  0.000000   \n",
      "59        Yes          2   16      0.4  0.0100  0.000000  0.000000  0.000000   \n",
      "174        No          2   64      0.2  0.0100  0.000000  0.000000  0.000000   \n",
      "64        Yes          2   32      0.0  0.0100  0.000000  0.000000  0.000000   \n",
      "69        Yes          2   32      0.2  0.0100  0.000000  0.000000  0.000000   \n",
      "79        Yes          2   64      0.0  0.0100  0.000000  0.000000  0.000000   \n",
      "169        No          2   64      0.0  0.0100  0.000000  0.000000  0.000000   \n",
      "149        No          2   16      0.4  0.0100  0.000000  0.000000  0.000000   \n",
      "84        Yes          2   64      0.2  0.0100  0.000000  0.000000  0.000000   \n",
      "89        Yes          2   64      0.4  0.0100  0.000000  0.000000  0.000000   \n",
      "164        No          2   32      0.4  0.0100  0.000000  0.000000  0.000000   \n",
      "139        No          2   16      0.0  0.0100  0.000000  0.000000  0.000000   \n",
      "144        No          2   16      0.2  0.0100  0.000000  0.000000  0.000000   \n",
      "159        No          2   32      0.2  0.0100  0.000000  0.000000  0.000000   \n",
      "179        No          2   64      0.4  0.0100  0.000000  0.000000  0.000000   \n",
      "\n",
      "      average  \n",
      "55   0.781867  \n",
      "147  0.850812  \n",
      "109  0.857212  \n",
      "94   0.860689  \n",
      "114  0.869075  \n",
      "19   0.816143  \n",
      "124  0.863628  \n",
      "4    0.843189  \n",
      "12   0.790050  \n",
      "34   0.816023  \n",
      "57   0.826399  \n",
      "168  0.778083  \n",
      "23   0.805536  \n",
      "129  0.788978  \n",
      "153  0.716770  \n",
      "99   0.761526  \n",
      "138  0.691258  \n",
      "134  0.740582  \n",
      "8    0.658105  \n",
      "158  0.632191  \n",
      "143  0.586176  \n",
      "119  0.514506  \n",
      "28   0.427492  \n",
      "178  0.450463  \n",
      "43   0.426676  \n",
      "13   0.338302  \n",
      "68   0.285893  \n",
      "39   0.278881  \n",
      "24   0.276541  \n",
      "53   0.278669  \n",
      "9    0.249910  \n",
      "163  0.251431  \n",
      "44   0.210896  \n",
      "78   0.207811  \n",
      "29   0.221515  \n",
      "83   0.207118  \n",
      "14   0.194256  \n",
      "104  0.153003  \n",
      "58   0.125905  \n",
      "73   0.123038  \n",
      "148  0.110160  \n",
      "88   0.037596  \n",
      "154  0.000000  \n",
      "74   0.000000  \n",
      "49   0.000000  \n",
      "54   0.000000  \n",
      "59   0.000000  \n",
      "174  0.000000  \n",
      "64   0.000000  \n",
      "69   0.000000  \n",
      "79   0.000000  \n",
      "169  0.000000  \n",
      "149  0.000000  \n",
      "84   0.000000  \n",
      "89   0.000000  \n",
      "164  0.000000  \n",
      "139  0.000000  \n",
      "144  0.000000  \n",
      "159  0.000000  \n",
      "179  0.000000  \n"
     ]
    }
   ],
   "source": [
    "print(df_fit[-60:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Embedding Num_layers Size  Dropout      Reg      Gen.     Prec.      Fit.  \\\n",
      "105        No          1   32      0.0  0.00000  0.008831  0.950646  0.958492   \n",
      "30        Yes          1   64      0.0  0.00000  0.005435  0.949031  0.956883   \n",
      "80        Yes          2   64      0.2  0.00000  0.000000  0.948833  0.956738   \n",
      "45        Yes          2   16      0.0  0.00000  0.001238  0.948781  0.956675   \n",
      "21        Yes          1   32      0.2  0.00001  0.050373  0.948354  0.955832   \n",
      "60        Yes          2   32      0.0  0.00000  0.002567  0.948313  0.956190   \n",
      "15        Yes          1   32      0.0  0.00000  0.001190  0.948052  0.955940   \n",
      "135        No          2   16      0.0  0.00000  0.001087  0.948000  0.955887   \n",
      "91         No          1   16      0.0  0.00001  0.125307  0.947740  0.954563   \n",
      "166        No          2   64      0.0  0.00001  0.031826  0.947719  0.955350   \n",
      "36        Yes          1   64      0.2  0.00001  0.045531  0.947698  0.955224   \n",
      "0         Yes          1   16      0.0  0.00000  0.055165  0.947646  0.955088   \n",
      "41        Yes          1   64      0.4  0.00001  0.085926  0.947635  0.954824   \n",
      "40        Yes          1   64      0.4  0.00000  0.000000  0.947406  0.955299   \n",
      "75        Yes          2   64      0.0  0.00000  0.003711  0.947344  0.955203   \n",
      "120        No          1   64      0.0  0.00000  0.001330  0.947292  0.955170   \n",
      "86        Yes          2   64      0.4  0.00001  0.521939  0.947240  0.950866   \n",
      "67        Yes          2   32      0.2  0.00010  0.807295  0.947104  0.948314   \n",
      "90         No          1   16      0.0  0.00000  0.015171  0.947073  0.954837   \n",
      "121        No          1   64      0.0  0.00001  0.160904  0.947000  0.953523   \n",
      "155        No          2   32      0.2  0.00000  0.000000  0.946885  0.954773   \n",
      "1         Yes          1   16      0.0  0.00001  0.081603  0.946615  0.953828   \n",
      "46        Yes          2   16      0.0  0.00001  0.079536  0.946594  0.953836   \n",
      "26        Yes          1   32      0.4  0.00001  0.047240  0.946469  0.953963   \n",
      "76        Yes          2   64      0.0  0.00001  0.034220  0.946396  0.954007   \n",
      "131        No          1   64      0.4  0.00001  0.564459  0.946313  0.949553   \n",
      "42        Yes          1   64      0.4  0.00010  0.717083  0.946292  0.948260   \n",
      "35        Yes          1   64      0.2  0.00000  0.004950  0.945927  0.953762   \n",
      "20        Yes          1   32      0.2  0.00000  0.000000  0.945677  0.953556   \n",
      "61        Yes          2   32      0.0  0.00001  0.030759  0.945667  0.953281   \n",
      "\n",
      "      average  \n",
      "105  0.639323  \n",
      "30   0.637116  \n",
      "80   0.635191  \n",
      "45   0.635565  \n",
      "21   0.651520  \n",
      "60   0.635690  \n",
      "15   0.635061  \n",
      "135  0.634991  \n",
      "91   0.675870  \n",
      "166  0.644965  \n",
      "36   0.649484  \n",
      "0    0.652633  \n",
      "41   0.662795  \n",
      "40   0.634235  \n",
      "75   0.635419  \n",
      "120  0.634597  \n",
      "86   0.806682  \n",
      "67   0.900904  \n",
      "90   0.639027  \n",
      "121  0.687143  \n",
      "155  0.633886  \n",
      "1    0.660682  \n",
      "46   0.659989  \n",
      "26   0.649224  \n",
      "76   0.644874  \n",
      "131  0.820108  \n",
      "42   0.870545  \n",
      "35   0.634880  \n",
      "20   0.633078  \n",
      "61   0.643236  \n"
     ]
    }
   ],
   "source": [
    "print(df_prec[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Embedding Num_layers Size  Dropout      Reg      Gen.     Prec.      Fit.  \\\n",
      "118        No          1   32      0.4  0.00100  0.844788  0.941958  0.942820   \n",
      "142        No          2   16      0.2  0.00010  0.834092  0.942375  0.943324   \n",
      "72        Yes          2   32      0.4  0.00010  0.829673  0.942302  0.943293   \n",
      "146        No          2   16      0.4  0.00001  0.818801  0.941875  0.942947   \n",
      "133        No          1   64      0.4  0.00100  0.817508  0.942365  0.943482   \n",
      "67        Yes          2   32      0.2  0.00010  0.807295  0.947104  0.948314   \n",
      "98         No          1   16      0.2  0.00100  0.803452  0.941188  0.942410   \n",
      "113        No          1   32      0.2  0.00100  0.797841  0.941927  0.943197   \n",
      "114        No          1   32      0.2  0.01000  0.780081  0.913000  0.914145   \n",
      "56        Yes          2   16      0.4  0.00001  0.779318  0.942427  0.943849   \n",
      "\n",
      "      average  \n",
      "118  0.909855  \n",
      "142  0.906597  \n",
      "72   0.905089  \n",
      "146  0.901208  \n",
      "133  0.901118  \n",
      "67   0.900904  \n",
      "98   0.895683  \n",
      "113  0.894322  \n",
      "114  0.869075  \n",
      "56   0.888531  \n"
     ]
    }
   ],
   "source": [
    "print(df_gen[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Embedding Num_layers Size  Dropout   Reg      Gen.     Prec.      Fit.  \\\n",
      "125        No          1   64      0.2  0.00  0.004630  0.945250  0.953095   \n",
      "70        Yes          2   32      0.4  0.00  0.004630  0.939375  0.947172   \n",
      "110        No          1   32      0.2  0.00  0.004173  0.943417  0.951246   \n",
      "75        Yes          2   64      0.0  0.00  0.003711  0.947344  0.955203   \n",
      "60        Yes          2   32      0.0  0.00  0.002567  0.948313  0.956190   \n",
      "130        No          1   64      0.4  0.00  0.001374  0.944479  0.952338   \n",
      "120        No          1   64      0.0  0.00  0.001330  0.947292  0.955170   \n",
      "65        Yes          2   32      0.2  0.00  0.001238  0.944344  0.952199   \n",
      "45        Yes          2   16      0.0  0.00  0.001238  0.948781  0.956675   \n",
      "15        Yes          1   32      0.0  0.00  0.001190  0.948052  0.955940   \n",
      "150        No          2   32      0.0  0.00  0.001190  0.943875  0.951726   \n",
      "25        Yes          1   32      0.4  0.00  0.001190  0.942656  0.950501   \n",
      "135        No          2   16      0.0  0.00  0.001087  0.948000  0.955887   \n",
      "74        Yes          2   32      0.4  0.01  0.000000  0.000000  0.000000   \n",
      "49        Yes          2   16      0.0  0.01  0.000000  0.000000  0.000000   \n",
      "139        No          2   16      0.0  0.01  0.000000  0.000000  0.000000   \n",
      "20        Yes          1   32      0.2  0.00  0.000000  0.945677  0.953556   \n",
      "89        Yes          2   64      0.4  0.01  0.000000  0.000000  0.000000   \n",
      "175        No          2   64      0.4  0.00  0.000000  0.941760  0.949606   \n",
      "174        No          2   64      0.2  0.01  0.000000  0.000000  0.000000   \n",
      "144        No          2   16      0.2  0.01  0.000000  0.000000  0.000000   \n",
      "85        Yes          2   64      0.4  0.00  0.000000  0.942094  0.949944   \n",
      "40        Yes          1   64      0.4  0.00  0.000000  0.947406  0.955299   \n",
      "170        No          2   64      0.2  0.00  0.000000  0.943635  0.951499   \n",
      "169        No          2   64      0.0  0.01  0.000000  0.000000  0.000000   \n",
      "84        Yes          2   64      0.2  0.01  0.000000  0.000000  0.000000   \n",
      "80        Yes          2   64      0.2  0.00  0.000000  0.948833  0.956738   \n",
      "154        No          2   32      0.0  0.01  0.000000  0.000000  0.000000   \n",
      "165        No          2   64      0.0  0.00  0.000000  0.944542  0.952409   \n",
      "164        No          2   32      0.4  0.01  0.000000  0.000000  0.000000   \n",
      "54        Yes          2   16      0.2  0.01  0.000000  0.000000  0.000000   \n",
      "59        Yes          2   16      0.4  0.01  0.000000  0.000000  0.000000   \n",
      "79        Yes          2   64      0.0  0.01  0.000000  0.000000  0.000000   \n",
      "160        No          2   32      0.4  0.00  0.000000  0.940302  0.948133   \n",
      "159        No          2   32      0.2  0.01  0.000000  0.000000  0.000000   \n",
      "64        Yes          2   32      0.0  0.01  0.000000  0.000000  0.000000   \n",
      "149        No          2   16      0.4  0.01  0.000000  0.000000  0.000000   \n",
      "69        Yes          2   32      0.2  0.01  0.000000  0.000000  0.000000   \n",
      "155        No          2   32      0.2  0.00  0.000000  0.946885  0.954773   \n",
      "179        No          2   64      0.4  0.01  0.000000  0.000000  0.000000   \n",
      "\n",
      "      average  \n",
      "125  0.634325  \n",
      "70   0.630392  \n",
      "110  0.632945  \n",
      "75   0.635419  \n",
      "60   0.635690  \n",
      "130  0.632730  \n",
      "120  0.634597  \n",
      "65   0.632593  \n",
      "45   0.635565  \n",
      "15   0.635061  \n",
      "150  0.632264  \n",
      "25   0.631449  \n",
      "135  0.634991  \n",
      "74   0.000000  \n",
      "49   0.000000  \n",
      "139  0.000000  \n",
      "20   0.633078  \n",
      "89   0.000000  \n",
      "175  0.630455  \n",
      "174  0.000000  \n",
      "144  0.000000  \n",
      "85   0.630679  \n",
      "40   0.634235  \n",
      "170  0.631711  \n",
      "169  0.000000  \n",
      "84   0.000000  \n",
      "80   0.635191  \n",
      "154  0.000000  \n",
      "165  0.632317  \n",
      "164  0.000000  \n",
      "54   0.000000  \n",
      "59   0.000000  \n",
      "79   0.000000  \n",
      "160  0.629478  \n",
      "159  0.000000  \n",
      "64   0.000000  \n",
      "149  0.000000  \n",
      "69   0.000000  \n",
      "155  0.633886  \n",
      "179  0.000000  \n"
     ]
    }
   ],
   "source": [
    "print(df_gen[-40:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Embedding Num_layers Size  Dropout      Reg      Gen.     Prec.      Fit.  \\\n",
      "118        No          1   32      0.4  0.00100  0.844788  0.941958  0.942820   \n",
      "142        No          2   16      0.2  0.00010  0.834092  0.942375  0.943324   \n",
      "72        Yes          2   32      0.4  0.00010  0.829673  0.942302  0.943293   \n",
      "146        No          2   16      0.4  0.00001  0.818801  0.941875  0.942947   \n",
      "133        No          1   64      0.4  0.00100  0.817508  0.942365  0.943482   \n",
      "67        Yes          2   32      0.2  0.00010  0.807295  0.947104  0.948314   \n",
      "98         No          1   16      0.2  0.00100  0.803452  0.941188  0.942410   \n",
      "113        No          1   32      0.2  0.00100  0.797841  0.941927  0.943197   \n",
      "56        Yes          2   16      0.4  0.00001  0.779318  0.942427  0.943849   \n",
      "128        No          1   64      0.2  0.00100  0.778264  0.940573  0.942000   \n",
      "\n",
      "      average  \n",
      "118  0.909855  \n",
      "142  0.906597  \n",
      "72   0.905089  \n",
      "146  0.901208  \n",
      "133  0.901118  \n",
      "67   0.900904  \n",
      "98   0.895683  \n",
      "113  0.894322  \n",
      "56   0.888531  \n",
      "128  0.886946  \n"
     ]
    }
   ],
   "source": [
    "print(df_average[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Results_grid_search.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Size', ylabel='Prec.'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYP0lEQVR4nO3dfZBc1X3m8e8zLWkkS0SSRzKFNSJSIsUOOEg2XcRYsYMxbMl2CiUxYPBScTYkSjb2lnfXRCaO1+UlcSr4beOkyAvGTig7KVBQYpQNG4INZLfiRNbIFnIkFlvLSxhBglAERopmNC+//aNvWz3DTE/3qG/f2/c+nyrQ3NNXPedqevrp83LPUURgZmbl1Zd1BczMLFsOAjOzknMQmJmVnIPAzKzkHARmZiW3IOsKtGvVqlWxbt26rKthZtZT9u3b93xErJ7psZ4LgnXr1jE0NJR1NczMeoqkp2Z7zF1DZmYl5yAwMys5B4GZWck5CMzMSs5BYGZWcg4Cy9SxE6M88vQLHDsxmnVVzEqr56aPztdNd3+DBx59jit/+FV86t1vyLo6Bty7/wgfuGv/945/57rNXLV5TXYVMsuxKz71IIefP8WGVUv4yk2Xd/S51WvLUFer1Wj3PoJ1N//Vy8qe/K13dqpKNg/HToxy8W985WXl+z5yBQPL+jOokVl+deI9TNK+iKjO9Fjhu4ZuuvsbbZVbd/zyF2cO89nKzcrqik892Fb5fBQ+CO755rNtlVt37HnqhbbKzcrq8POn2iqfj8IHgZmZNVf4IHjFwvbKrTuWzvLvP1u5WVmdd87MvxSzlc9H4YPgTT8442J7s5Zbd7x3yw+0VW5WVu/cNNhW+XwUPgjOXb6krXLrjstf86q2ys3KauuF57ZVPh+FD4LLXzPzJ//Zyq07Fi6otFVuVlbV9QO8ecPAlLI3bxigun5glr/RvsIHwamxybbKrTuef2mkrXKzMru6upZFlT4WVcSiSh/XVNd29PkLHwRPHTvZVrl1xyPDL7ZVblZWx06M8qFdBzg9McnpieD0xCQ7dh3o6LIshQ+CVy5d1Fa5dcdbNq5qq9ysrIaPz3y/wGzl81H4IFizcuZB4dnKrTu60e9pVgRLF1UYmdaVPTI2ydJFnRtPK/yicxe+ejl9gsmGJZX6VCu3bH3x59/I0BPH+N/feZ63bFzlEDCbwcnTE/RXxOjEmTex/oo4eXqiY9+j8EEAUOkTkw3/iJU+ZVgba1Rd71aAWTODK5egPkHDe5j6xGAHezUK3zU0fPwUi6dNSVy8oNLR/jUzs7QMLOvnE++6iMUL+zinfwGLF/bxiXdd1NFVegvfIhhcuYSxyan9a2OTkx1NUzOzNF21eQ1bNqxi+PgpBlcu6fhS7YVvEXQjTc3MelnhWwSQfpqamaXp3v1H+NCuAyzs62NscpJPvOuiju7mV/gWQd3Asn42rV3hEMgZ71ls1lz9hrKRsUleGh1nZKzzN5SVokVg+ZT2pxyzIhg+foqFfX2McGasc2FfH8PHT3Xsg21pWgSWL934lGNWBN2Y8OIgsEzUP+U0qn/KMbMzPH3UCsvTes1a5+mjVkie1mvWnjQnvLhFYJnxtF6zfHAQWKYGlvU7AMwy5q4hM7OScxCYmfWANG++dNeQZerYiVGPEZjNoaeXmJC0VdJjkg5LunmGx8+X9JCkb0o6IOkdadbH8uXe/UfYcuuD3HDHHrbc+iC79x/JukpmudONmy9TCwJJFeA24O3ABcD1ki6YdtpHgJ0R8XrgOuD30qqP5YvvLDZrTTduvkyzRXAJcDgiHo+I08BdwLZp5wTwfcnXy4FnUqyP5YjvLDZrTa8vMbEGeLrheDgpa/Qx4AZJw8B9wH9KsT6WI76z2Kw1ZVhi4nrgjyPi05IuBb4o6XURMeUdQtJ2YDvA+eefn0E1rdPqL+4d0wbAPGBs9nJp33yZZhAcAdY2HA8mZY1uBLYCRMTfS1oMrAKeazwpIm4HbgeoVquBFYLvLDZrXZo3X6bZNbQX2ChpvaRF1AaDd08755+AtwFI+mFgMXA0jcp4A5R88oZBZtlLrUUQEeOS3g/cD1SAL0TEQUm3AEMRsRv4IPA5Sf+F2sDxz0ZExz/x37v/CDvueYSK+piIST559SZvgGJmllAK77upqlarMTQ01PL5x06M8qO/+RXGG0YdFvTBng9f4U+hZlYakvZFRHWmxwq/xMTBZ747JQQAxidr5WZmVoIgqPU4tVNuZlYuhQ+CC1+9nIUVTSlbWBEXvnp5RjUyM2ufF507CwPL+vn0NZv4lXsOUOkTE5PBJ6/2fHUz6x1pLzpX+CAAz1c3s97VuC7XCLUBzx27DrBlw6qOvZeVIgjAO2GZWW+qr8tVDwE4sy5Xp97TCj9GYGbWy3p90TkzMztLZVh0zszM5tDLi86ZmVmH9Oqic2Zm1gNKEwRefdTMbGal6BpK+2YMM7NeVvgWgTdJNzNrrvBB4E3SzcyaK3wQeJP0fPPYjVn2Cj9G4E3S88tjN2b5UPggAC86l0fdWEjLzFpTiiAALzqXN91YSMvMWlP4MQLLJ4/dmOWHg8Ay0Y2FtMysNaXpGrL88diNWT44CCxTHrsxy567hszMSs5BYGZWcg4CM7OScxCYmZWcg8DMrORKEwRe3MzMbGalmD7qxc3MzGZX+BaBN6YxM2uu8EHgjWnMzJorfBB4cTMzs+ZSDQJJWyU9JumwpJtnOedaSYckHZT0p52ugxc3MzNrLrXBYkkV4DbgSmAY2Ctpd0QcajhnI/CrwJaIOC7pVWnUxYubmZnNLs1ZQ5cAhyPicQBJdwHbgEMN5/wCcFtEHAeIiOfSqowXNzMzm1maXUNrgKcbjoeTskY/BPyQpL+T9A+Sts70RJK2SxqSNHT06NGUqmtmVk5ZDxYvADYClwHXA5+TtGL6SRFxe0RUI6K6evXq7tbQzKzg0gyCI8DahuPBpKzRMLA7IsYi4gng29SCwczMuiTNINgLbJS0XtIi4Dpg97RzvkytNYCkVdS6ih5PsU5mZjZNakEQEePA+4H7gUeBnRFxUNItkq5KTrsfOCbpEPAQ8CsRcSytOpmZ2cspIrKuQ1uq1WoMDQ1lXQ0zs54iaV9EVGd6LOvBYjMzy5iDwMys5BwEZmYl5yAwMys5B4FlyjvHmWWvFDuUWT555zizfHCLwDLhnePM8sNBYJnwznFm+eEgsEx45ziz/HAQWCa8c5xZfniw2DLjnePM8mHeLQJJ2ztZESungWX9bFq7wiFglqGz6RpSx2phZmaZmXcQRMQfdrIiZmaWjZaCQNJvNm4hKWmlpN9IrVZmZtY1rbYI3h4RL9QPIuI48I5UamSl4iUmzLLX6qyhiqT+iBgFkLQE8OienRUvMWGWD622CP4E+KqkGyXdCDwA3JletazovMSEWXvSbD231CKIiFslPQJckRT9ekTc3/HaWGnUl5gY4czdxfUlJjyV1GyqtFvP7dxQ9igwHhFfkfQKSedExEsdq4mVipeYMGtNY+u5/sFpx64DbNmwqmMfmlqdNfQLwD1AfcroGuDLHamBlZKXmDBrzWwLMXZygcZWWwTvAy4B9gBExHckvapjtbBS8hITZnNbuqjCyNjU1vPI2CRLF1U69j1aDYLRiDgt1W4mlrQAiI7VwkprYFm/A8CsiZOnJ+iviNGJM2+5/RVx8vREx75Hq7OG/lbSh4Elkq4E/gz4y47VwszMZjS4cgnqm7qij/rU0fG0VoPgQ8BR4FvALwL3AR/pWC3MzGxG3RhPm7NrSFIFOBgRrwU+17HvbGZmLUl7PG3OIIiICUmPSTo/Iv6po9/dzMxakuZ4WquDxSuBg5K+DpysF0bEVanUyszMuqbVIPhvqdbCzMwy0zQIJC0GfgnYQG2g+PMRMd6NipmZWXfMNWvoTqBKLQTeDnw69RqZmVlXzdU1dEFE/AiApM8DX0+/SmZm1k1ztQjG6l+4S8jMrJjmCoJNkr6b/PcScFH9a0nfnevJJW1Npp4elnRzk/PeJSkkVdu9ADMzOztNu4YiYt6rGiU3ot0GXAkMA3sl7Y6IQ9POOwf4AMmCdmZm1l2tLjExH5cAhyPi8Yg4DdwFbJvhvF8HbgVGUqyLmZnNIs0gWAM83XA8nJR9j6Q3AGsj4q+aPZGk7ZKGJA0dPXq08zU1MyuxNIOgKUl9wGeAD851bkTcHhHViKiuXr06/cqZmZVImkFwBFjbcDyYlNWdA7wOeFjSk8Abgd1pDRinufGzmVkva2fP4nbtBTZKWk8tAK4D3lN/MCJeBFbVjyU9DNwUEUOdrkjaGz+bmfWy1FoEyX0H7wfup7bx/c6IOCjpFkldW6yucePnl0bHGRmbZMeuA24ZmJkl0mwREBH3UdvEprHso7Oce1kadRg+foqFfX2McGbPz4V9fQwfP+UtEs3MyHCwuFsGVy5hbHLqxs9jk5Md3ebNzKyXFT4IurHNm5lZL0u1aygv0t7mzcysl5UiCCDdbd7MzHpZ4buGzMysOQeBmVnJOQjMzErOQWBmVnIOAjOzknMQWKa8GKBZ9kozfdTyx4sBmuWDWwSWCS8GaNaeNFvPbhFYJrwYoFnr0m49u0VgmfBigGat6Ubr2UFgmRhY1s+11cEpZddWB90aMJum3npuVG89d0ppgsCzU/Ll2IlRdg4NTynbOTTsn4/ZNN1oPZciCO7df4Qttz7IDXfsYcutD7J7/5G5/5KlqhufcsyKoBtL6Rd+sLixf60+MLlj1wG2bFjlbogMeYzArHVpL6Vf+BbBbJ8w/ckzW94wyKw9A8v62bR2RSq/I4VvESxdVGFkbOonz5GxSZYuqmRUI6vzhkFm+VD4FsHJ0xP0VzSlrL8iTp6eyKhGZmb5UvgWweDKJahPMBHfK1Of3BedA15iwiwfCt8icF90PnmJCbP8KHyLANwXnUdeYsIsP0oRBODN6/PG00fN2nPsxGhqH2ZLEwSWL/Uuux3Txggc1mYvl/Z4moPAMnPV5jVccN73sf/pF9i8dgUbzj0n6yqZ5U43bop1EFhmPGvIbG7Dx08xNj6tG3V8sqPjaYWfNWT55FlDZq0ZG59onP0O1GbDj4137l4oB4FlwovOmbXmyWP/1lb5fDgILBOeNWTWms1rV7RVPh+lCQLvR5AvvtHPrDUbzj2Hn7n0/CllP3Pp+R2dXKGImPus+T65tBX4LFAB7oiI35r2+H8Ffh4YB44CPxcRTzV7zmq1GkNDQ23Vw4OS+ZXm3GizIjn8Ly+d1Qw7SfsiojrjY2kFgaQK8G3gSmAY2AtcHxGHGs55K7AnIv5N0n8ELouIdzd73naD4NiJUbbc+uCUFUgXL+zj7z50ud94zKw0mgVBml1DlwCHI+LxiDgN3AVsazwhIh6KiPqIxz8Ag3SYByXNzJpLMwjWAE83HA8nZbO5EfhfMz0gabukIUlDR48ebasSHpQ0M2suF4PFkm4AqsAnZ3o8Im6PiGpEVFevXt3Wc3tQ0sysuTTvLD4CrG04HkzKppB0BfBrwI9HRCpTerz6qJnZ7NIMgr3ARknrqQXAdcB7Gk+Q9HrgD4GtEfFcinXx6qNmZrNIrWsoIsaB9wP3A48COyPioKRbJF2VnPZJYBnwZ5L2S9qdVn3MzGxmqS46FxH3AfdNK/tow9dXpPn9zcxsbrkYLO4G31lsZjazUixD7TuLzcxmV/gWgZc7NjNrrvBB4DuLzcyaK3wQ+M5iM7PmCh8E9TuL+xf08YpFFfoX+M5iM7NGhQ8CgKj/P84cWT54NpdZ9go/a6g+WDw6HkBtj88duw6wZcMqtwoy5tlcZvlQ+BaBB4vzybO5zPKj8EHgweJ8ckCb5Ufhg2BgWT/XXjx1v5trq4PuFsqYA9osPwofBMdOjLJz3/CUsp1Dw+6CyJj3iTDLj8IPFte7IEY48+mz3gXhN51seZ8Is3wofBC4CyLfvE+EWfYK3zXkLggzs+YK3yIAd0GYmTVTiiAAd0GYmc2m8F1DZmbWnIPAzKzkHARmZiXnIDAzKzkHgZlZyTkIzMxKzkFgZlZyDgIzs5JzEJiZ9YA0t3UtzZ3FZma9Ku1tXd0iMDPLsW5s6+ogMDPLsW5s6+ogMDPLsW7sqeIgMDPLsW7sqeLBYjOznEt7TxUHgZlZD0hzT5VUu4YkbZX0mKTDkm6e4fF+SXcnj++RtC6tunzpa09wzR98jS997Ym0voXNg38uZq1J83dFEdHxJwWQVAG+DVwJDAN7gesj4lDDOb8MXBQRvyTpOuCnIuLdzZ63Wq3G0NBQW3XZ9LG/5sWRie8dL19c4ZGPbW3rOazz/HMxa00nflck7YuI6kyPpdkiuAQ4HBGPR8Rp4C5g27RztgF3Jl/fA7xNkjpZiS997Ykp/4AAL45M+BNoxvxzMWtNN35X0gyCNcDTDcfDSdmM50TEOPAiMDD9iSRtlzQkaejo0aNtVeLeA8+2VW7d4Z+LWWu68bvSE9NHI+L2iKhGRHX16tVt/d1tF53XVrl1h38uZq3pxu9KmkFwBFjbcDyYlM14jqQFwHLgWCcrccOb1rN8cWVK2fLFFW540/pOfhtrk38uZq3pxu9KmoPFC6gNFr+N2hv+XuA9EXGw4Zz3AT/SMFj80xFxbbPnnc9gMdT62e498CzbLjrPbzY54p+LWWvO9nel2WBxakGQfON3AL8NVIAvRMTHJd0CDEXEbkmLgS8Crwf+FbguIh5v9pzzDQIzszJrFgSp3lAWEfcB900r+2jD1yPANWnWwczMmuuJwWIzM0uPg8DMrOQcBGZmJecgMDMruVRnDaVB0lHgqazrMYdVwPNZVyIjvvbyKvP198K1f39EzHhHbs8FQS+QNDTbNK2i87WX89qh3Nff69furiEzs5JzEJiZlZyDIB23Z12BDPnay6vM19/T1+4xAjOzknOLwMys5BwEZmYl5yA4C5LWSnpI0iFJByV9ICl/paQHJH0n+XNl1nVNg6TFkr4u6ZHk+v97Ur5e0h5JhyXdLWlR1nVNi6SKpG9K+p/JcSmuXdKTkr4lab+koaSsFK97AEkrJN0j6f9KelTSpb18/Q6CszMOfDAiLgDeCLxP0gXAzcBXI2Ij8NXkuIhGgcsjYhOwGdgq6Y3ArcD/iIgNwHHgxuyqmLoPAI82HJfp2t8aEZsb5s+X5XUP8FngryPitcAmaq+Bnr1+B8FZiIhnI+IbydcvUXsxrAG2AXcmp90J/GQmFUxZ1JxIDhcm/wVwOXBPUl7Y65c0CLwTuCM5FiW59lmU4nUvaTnwFuDzABFxOiJeoIev30HQIZLWUdtgZw9wbkTUd5b+Z+DcrOqVtqRrZD/wHPAA8P+AFyJiPDllmFo4FtFvAzuAyeR4gPJcewB/I2mfpO1JWVle9+uBo8AfJd2Cd0haSg9fv4OgAyQtA3YB/zkivtv4WNTm5xZ2jm5ETETEZmp7Ul8CvDbbGnWHpJ8AnouIfVnXJSM/FhFvAN5OrUv0LY0PFvx1vwB4A/D7EfF64CTTuoF67fodBGdJ0kJqIfAnEfHnSfG/SDovefw8ap+WCy1pGj8EXAqsSPashlpAHMmqXinaAlwl6UngLmpdQp+lHNdORBxJ/nwO+AtqHwLK8rofBoYjYk9yfA+1YOjZ63cQnIWkT/jzwKMR8ZmGh3YD702+fi9wb7fr1g2SVktakXy9BLiS2jjJQ8DVyWmFvP6I+NWIGIyIdcB1wIMR8e8pwbVLWirpnPrXwL8D/pGSvO4j4p+BpyW9Jil6G3CIHr5+31l8FiT9GPB/gG9xpp/4w9TGCXYC51NbMvvaiPjXTCqZIkkXURsUq1D7ULEzIm6R9APUPiW/EvgmcENEjGZX03RJugy4KSJ+ogzXnlzjXySHC4A/jYiPSxqgBK97AEmbqU0SWAQ8DvwHkt8BevD6HQRmZiXnriEzs5JzEJiZlZyDwMys5BwEZmYl5yAwMys5B4FZiyT9WrLK6oFk1c0fTZYXuCDrupmdDU8fNWuBpEuBzwCXRcSopFXAooh4JuOqmZ01twjMWnMe8Hz95rCIeD4inpH0sKSqpKuSVsJ+SY9JegJA0sWS/jZZnO3++hIEZnniIDBrzd8AayV9W9LvSfrxxgcjYneyNv9m4BHgU8k6VL8LXB0RFwNfAD7e7YqbzWXB3KeYWUSckHQx8GbgrcDdkl628YikHcCpiLhN0uuA1wEP1JalogI8O/3vmGXNQWDWooiYAB4GHpb0Lc4sMAaApCuAa6htWgIg4GBEXNrNepq1y11DZi2Q9BpJGxuKNlNbWKz++PcDtwHXRMSppPgxYHUy0IykhZIu7FKVzVrmFoFZa5YBv5ssuz0OHAa2c2Zbyp+ltkPZl5NuoGci4h2SrgZ+J9necAG1Xc0OdrXmZnPw9FEzs5Jz15CZWck5CMzMSs5BYGZWcg4CM7OScxCYmZWcg8DMrOQcBGZmJff/AepU03OZUkt1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot('Size', 'Prec.', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
